{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7e2562",
   "metadata": {},
   "source": [
    "# Grammar Scoring Engine - V5 Knowledge Distillation Inference\n",
    "\n",
    "**Competition:** SHL Intern Hiring Assessment 2025\n",
    "\n",
    "**Author:** Uday Bhatia\n",
    "\n",
    "**Model Version:** V5 (Knowledge Distillation from V2)\n",
    "\n",
    "**Date:** November 9, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "V5 uses **knowledge distillation** to combine the best of V2 and V4:\n",
    "- **Teacher:** V2 models (proven 0.533 test RMSE, correct distribution)\n",
    "- **Student:** V4 comparative architecture (powerful but previously overfit)\n",
    "- **Training:** Learn from V2's predictions + true labels + comparative pairs\n",
    "- **Result:** Best of both worlds!\n",
    "\n",
    "### Training Performance (V5)\n",
    "\n",
    "**OOF RMSE:** 0.2603\n",
    "\n",
    "**Pearson Correlation:** 0.9412\n",
    "\n",
    "### Comparison with Previous Versions\n",
    "\n",
    "| Version | Strategy | OOF RMSE | Improvement |\n",
    "|---------|----------|----------|-------------|\n",
    "| V2 | Enhanced LoRA | 0.5380 | Baseline |\n",
    "| V4 | Comparative | 0.5106 | +5.1% |\n",
    "| **V5** | **Distillation** | **0.2603** | **+51.6%!** |\n",
    "\n",
    "### Why V5 Works\n",
    "\n",
    "1. **V2's Wisdom:** Learns from V2's predictions (soft targets with correct distribution)\n",
    "2. **V4's Capacity:** Powerful comparative learning architecture\n",
    "3. **Best of Both:** Combines generalization + capacity\n",
    "4. **Temperature Scaling:** Softer targets (T=3.0) for better knowledge transfer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecafaeab",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Install required packages and load libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e03d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java 17 (required for LanguageTool)\n",
    "!apt-get update -qq\n",
    "!apt-get install -y openjdk-17-jdk-headless > /dev/null 2>&1\n",
    "!update-alternatives --set java /usr/lib/jvm/java-17-openjdk-amd64/bin/java\n",
    "\n",
    "# Verify Java version\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaadd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q numpy==1.23.5 scipy==1.10.1\n",
    "!pip install -q transformers==4.44.0 peft==0.12.0 accelerate sentencepiece protobuf\n",
    "!pip install -q faster-whisper language-tool-python textstat\n",
    "!pip install -q spacy==3.7.5\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from faster_whisper import WhisperModel\n",
    "import spacy\n",
    "import language_tool_python\n",
    "import textstat\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    BASE_DIR = Path('/kaggle/input/shl-intern-hiring-assessment-2025/dataset')\n",
    "    MODEL_DIR = Path('/kaggle/input/grammar-scoring-models-v5-distillation')\n",
    "    CACHE_DIR = Path('/kaggle/working/cache')\n",
    "    print(\"Running on Kaggle\")\n",
    "else:\n",
    "    BASE_DIR = Path('/home/azureuser/shl2/dataset')\n",
    "    MODEL_DIR = Path('/home/azureuser/shl2/v5_distillation/models')\n",
    "    CACHE_DIR = Path('/home/azureuser/shl2/cache')\n",
    "    print(\"Running locally\")\n",
    "\n",
    "DATA_DIR = BASE_DIR / 'csvs'\n",
    "AUDIO_DIR = BASE_DIR / 'audios'\n",
    "CACHE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"AUDIO_DIR: {AUDIO_DIR}\")\n",
    "print(f\"MODEL_DIR: {MODEL_DIR}\")\n",
    "print(f\"CACHE_DIR: {CACHE_DIR}\")\n",
    "\n",
    "# Load test data only\n",
    "test_df = pd.read_csv(DATA_DIR / 'test.csv')\n",
    "\n",
    "print(f\"\\n✓ Test samples: {len(test_df)}\")\n",
    "print(f\"\\nTest columns: {test_df.columns.tolist()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711b51e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Architecture (V5 Distillation)\n",
    "\n",
    "### Knowledge Distillation Strategy\n",
    "\n",
    "**V5 = V4 Architecture + V2 Knowledge**\n",
    "\n",
    "**Training Process:**\n",
    "1. Load V2 teacher models (5 folds)\n",
    "2. Generate V2 predictions on training data (soft targets)\n",
    "3. Train V4 student with 3 objectives:\n",
    "   - **Distillation Loss (50%):** Match V2's predictions\n",
    "   - **Hard Label Loss (30%):** Learn from true labels\n",
    "   - **Comparative Loss (20%):** Learn pairwise relationships\n",
    "\n",
    "**Key Parameters:**\n",
    "- Temperature: 3.0 (softer targets for better knowledge transfer)\n",
    "- 12 epochs with alternating training (single samples ↔ pairs)\n",
    "- Early stopping when validation stops improving\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "Same as V4:\n",
    "1. **Encoder:** DeBERTa-v3-large with LoRA (r=16, α=32, 6 layers)\n",
    "2. **Pooling:** Mean pooling\n",
    "3. **Absolute Head:** 3-layer MLP for single text scoring\n",
    "4. **Comparative Head:** For pairwise learning (training only)\n",
    "\n",
    "**Inference:** Uses only absolute head (single text → score)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b01cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define V5 model architecture\n",
    "\n",
    "class MeanPool(nn.Module):\n",
    "    \"\"\"Mean pooling over sequence dimension.\"\"\"\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).float()\n",
    "        return (last_hidden_state * mask_expanded).sum(1) / mask_expanded.sum(1).clamp(min=1.0)\n",
    "\n",
    "class V5StudentModel(nn.Module):\n",
    "    \"\"\"V5: Distilled from V2 teacher with V4 architecture.\"\"\"\n",
    "    def __init__(self, model_name='microsoft/deberta-v3-large', dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Attach LoRA to top 6 layers\n",
    "        self._attach_lora_top_layers(last_n_layers=6)\n",
    "        \n",
    "        # Freeze non-LoRA parameters\n",
    "        for n, p in self.encoder.named_parameters():\n",
    "            if \"lora_\" not in n:\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.pool = MeanPool()\n",
    "        \n",
    "        # Absolute score head (used for inference)\n",
    "        self.absolute_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 2048),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # Comparative head (used during training only)\n",
    "        self.comparative_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 3, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def _attach_lora_top_layers(self, last_n_layers=6):\n",
    "        \"\"\"Apply LoRA to top 6 layers with r=16, alpha=32.\"\"\"\n",
    "        n_layers = len(self.encoder.encoder.layer)\n",
    "        keep_layers = set(range(n_layers - last_n_layers, n_layers))\n",
    "        \n",
    "        target_modules = []\n",
    "        for i in keep_layers:\n",
    "            target_modules.extend([\n",
    "                f\"encoder.layer.{i}.attention.self.query_proj\",\n",
    "                f\"encoder.layer.{i}.attention.self.key_proj\",\n",
    "                f\"encoder.layer.{i}.attention.self.value_proj\"\n",
    "            ])\n",
    "        \n",
    "        cfg = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            target_modules=target_modules,\n",
    "            modules_to_save=[]\n",
    "        )\n",
    "        \n",
    "        self.encoder = get_peft_model(self.encoder, cfg)\n",
    "    \n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        \"\"\"Encode a single text into embedding\"\"\"\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self.pool(outputs.last_hidden_state, attention_mask)\n",
    "        return pooled\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \"\"\"Forward pass for single text (inference mode).\"\"\"\n",
    "        emb = self.encode(batch['input_ids'], batch['attention_mask'])\n",
    "        return self.absolute_head(emb).squeeze(-1)\n",
    "\n",
    "print(\"✅ V5 Distillation model architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75692507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all 5 V5 distilled models\n",
    "print(\"Loading V5 distilled models...\")\n",
    "text_models = []\n",
    "for fold in range(5):\n",
    "    model = V5StudentModel()\n",
    "    state_dict = torch.load(MODEL_DIR / f'model_fold{fold}_best.pth', map_location=DEVICE)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    text_models.append(model)\n",
    "    print(f\"  ✓ Fold {fold} loaded\")\n",
    "\n",
    "tokenizer = text_models[0].tok\n",
    "print(\"✅ All V5 distilled models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd5730",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Audio Transcription\n",
    "\n",
    "Convert test audio files to text using faster-whisper.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio_files(df, audio_dir, cache_path=None):\n",
    "    \"\"\"Transcribe audio files using faster-whisper.\"\"\"\n",
    "    if cache_path and os.path.exists(cache_path):\n",
    "        print(f\"Loading cached transcripts from {cache_path}\")\n",
    "        cached = pd.read_csv(cache_path)\n",
    "        return cached\n",
    "    \n",
    "    print(\"Initializing Whisper model (large-v3)...\")\n",
    "    whisper = WhisperModel(\n",
    "        \"large-v3\",\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        compute_type=\"float16\" if torch.cuda.is_available() else \"float32\"\n",
    "    )\n",
    "    \n",
    "    transcripts = []\n",
    "    print(f\"Transcribing {len(df)} audio files...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        audio_path = Path(audio_dir) / f\"{row['filename']}.wav\"\n",
    "        segments, info = whisper.transcribe(str(audio_path), beam_size=5, language=\"en\")\n",
    "        text = \" \".join([seg.text for seg in segments])\n",
    "        transcripts.append(text.strip())\n",
    "        \n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(df)} files\")\n",
    "    \n",
    "    df['transcript'] = transcripts\n",
    "    \n",
    "    if cache_path:\n",
    "        df.to_csv(cache_path, index=False)\n",
    "        print(f\"✓ Cached transcripts to {cache_path}\")\n",
    "    \n",
    "    del whisper\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"✅ Transcription function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c5c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory before transcription\n",
    "print(\"Freeing GPU memory...\")\n",
    "del text_models\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"✓ GPU memory cleared\\n\")\n",
    "\n",
    "# Transcribe test data\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSCRIBING TEST DATA\")\n",
    "print(\"=\" * 60)\n",
    "test_df = transcribe_audio_files(\n",
    "    test_df,\n",
    "    AUDIO_DIR / 'test',\n",
    "    CACHE_DIR / 'test_transcripts.csv'\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Transcription complete\")\n",
    "print(f\"\\nSample transcripts:\")\n",
    "for i in range(min(3, len(test_df))):\n",
    "    print(f\"\\n[{i+1}] {test_df.iloc[i]['transcript'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27785fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload V5 distilled models for inference\n",
    "print(\"Reloading V5 distilled models for inference...\")\n",
    "text_models = []\n",
    "for fold in range(5):\n",
    "    model = V5StudentModel()\n",
    "    state_dict = torch.load(MODEL_DIR / f'model_fold{fold}_best.pth', map_location=DEVICE)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    text_models.append(model)\n",
    "    print(f\"  ✓ Fold {fold} loaded\")\n",
    "\n",
    "tokenizer = text_models[0].tok\n",
    "print(\"✅ V5 distilled models reloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297bc516",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Inference Pipeline\n",
    "\n",
    "Run predictions using V5 distilled models and ensemble them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac4502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "print(\"✅ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d81897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_v5(df, models, tokenizer):\n",
    "    \"\"\"Generate predictions from V5 distilled models (all 5 folds).\"\"\"\n",
    "    all_preds = np.zeros((len(df), 5))\n",
    "    \n",
    "    for fold, model in enumerate(models):\n",
    "        print(f\"  Fold {fold}: Predicting...\")\n",
    "        \n",
    "        dataset = TextDataset(df['transcript'].values, np.zeros(len(df)), tokenizer)\n",
    "        loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "        \n",
    "        fold_preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                attn_mask = batch['attention_mask'].to(DEVICE)\n",
    "                \n",
    "                with autocast(dtype=torch.bfloat16):\n",
    "                    preds = model({'input_ids': input_ids, 'attention_mask': attn_mask})\n",
    "                \n",
    "                fold_preds.append(preds.float().cpu())\n",
    "        \n",
    "        fold_preds = torch.cat(fold_preds).numpy()\n",
    "        all_preds[:, fold] = fold_preds\n",
    "        \n",
    "        print(f\"    ✓ Complete (mean: {fold_preds.mean():.3f}, std: {fold_preds.std():.3f})\")\n",
    "    \n",
    "    return all_preds.mean(axis=1)\n",
    "\n",
    "print(\"✅ Prediction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835ad174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on TEST data\n",
    "print(\"=\" * 60)\n",
    "print(\"PREDICTING ON TEST DATA (V5 Distilled Models)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nV5 Distillation (V2 Teacher + V4 Architecture):\")\n",
    "test_final_preds = predict_v5(test_df, text_models, tokenizer)\n",
    "\n",
    "print(f\"\\n  ✓ Final predictions (mean: {test_final_preds.mean():.3f}, std: {test_final_preds.std():.3f})\")\n",
    "print(\"\\n✅ Test predictions complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455dbc1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Generation\n",
    "\n",
    "Create final submission file for Kaggle.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'filename': test_df['filename'],\n",
    "    'label': test_final_preds\n",
    "})\n",
    "\n",
    "# Clip predictions to valid range [0, 5]\n",
    "submission['label'] = submission['label'].clip(0, 5)\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"V5 KNOWLEDGE DISTILLATION SUBMISSION CREATED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"File: /kaggle/working/submission.csv\")\n",
    "print(f\"Samples: {len(submission)}\")\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(submission['label'].describe())\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "print(submission.head(10))\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n✅ Ready to submit!\")\n",
    "print(\"\\nV5 Training Performance:\")\n",
    "print(f\"  OOF RMSE: 0.2603 (51.6% better than V2!)\")\n",
    "print(f\"  Pearson: 0.9412\")\n",
    "print(f\"\\n  V2 (teacher): 0.5380\")\n",
    "print(f\"  V4 (student): 0.5106\")\n",
    "print(f\"  V5 (distilled): 0.2603 ⭐\")\n",
    "print(f\"\\nKnowledge Distillation = Best of Both Worlds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1432744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test predictions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(test_final_preds, bins=30, alpha=0.7, color='gold', edgecolor='black')\n",
    "plt.xlabel('Predicted Grammar Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('V5 Knowledge Distillation - Test Set Prediction Distribution')\n",
    "plt.axvline(test_final_preds.mean(), color='red', linestyle='--', label=f'Mean: {test_final_preds.mean():.3f}')\n",
    "plt.axvline(np.median(test_final_preds), color='blue', linestyle='--', label=f'Median: {np.median(test_final_preds):.3f}')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('/kaggle/working/test_predictions_v5.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ V5 test prediction visualization saved\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
